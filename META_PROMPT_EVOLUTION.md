# Meta Prompt Evolution in OpenEvolve

This document describes the meta prompt evolution feature implemented in OpenEvolve, inspired by the AlphaEvolve research paper.

## Overview

Meta prompt evolution is a feature that automatically evolves and improves the system prompts used to guide the LLM during code evolution. Instead of using a fixed system prompt throughout the evolution process, the system continuously generates better prompts based on performance feedback from the programs they helped create.

### Key Benefits

- **Adaptive Prompting**: System prompts evolve to become more effective over time
- **Performance Improvement**: Better prompts lead to better generated programs
- **Automated Optimization**: No manual prompt engineering required during evolution
- **Context Awareness**: Prompts are optimized based on actual performance data

## Architecture

The meta prompt evolution system consists of several key components:

### 1. MetaPromptDatabase

Stores and manages evolved system prompts similar to how the main database stores programs.

- **Storage**: Maintains a collection of meta prompts with performance metrics
- **Sampling**: Provides weighted selection of meta prompts based on effectiveness
- **Tracking**: Links generated programs to the meta prompts that created them

### 2. MetaPromptEvolver

Uses an LLM to generate improved system prompts based on performance feedback.

- **Analysis**: Analyzes current prompt performance and top-performing programs
- **Generation**: Creates new system prompts that address identified weaknesses
- **Evolution**: Iteratively improves prompts over multiple generations

### 3. Integration Points

The system integrates seamlessly with the existing OpenEvolve architecture:

- **PromptSampler**: Modified to use evolved meta prompts when available
- **ProcessParallelController**: Tracks which meta prompt was used for each program
- **Controller**: Orchestrates periodic meta prompt evolution

## How It Works

1. **Initialization**: Starts with the base system message as the initial meta prompt
2. **Program Generation**: Programs are generated using sampled meta prompts
3. **Performance Tracking**: Links each program to the meta prompt that helped create it
4. **Meta Prompt Evolution**: Periodically evolves meta prompts based on performance feedback
5. **Continuous Improvement**: Better meta prompts lead to better programs over time

### Evolution Process

Every N iterations (configurable), the system:

1. **Analyzes Performance**: Examines which meta prompts generated the best programs
2. **Gathers Feedback**: Collects metrics and patterns from top-performing programs  
3. **Generates New Prompts**: Uses an LLM to create improved system prompts
4. **Updates Database**: Adds new meta prompts to the selection pool

## Configuration

Enable meta prompt evolution in your config:

```yaml
prompt:
  # Enable meta prompt evolution
  use_meta_prompting: true
  meta_prompt_weight: 0.1
  meta_prompt_evolution_interval: 20  # Evolve every 20 iterations
  
  # Base system message (used as initial meta prompt)
  system_message: "Your base system prompt here..."
```

### Configuration Parameters

- `use_meta_prompting`: Enable/disable meta prompt evolution (default: false)
- `meta_prompt_weight`: Weight for meta prompt selection (currently unused)
- `meta_prompt_evolution_interval`: How often to evolve meta prompts (default: 20)

## Usage Example

```python
from openevolve import OpenEvolve
from openevolve.config import Config

# Create config with meta prompt evolution enabled
config = Config()
config.prompt.use_meta_prompting = True
config.prompt.meta_prompt_evolution_interval = 20
config.prompt.system_message = "Your initial system prompt..."

# Run evolution with meta prompt evolution
controller = OpenEvolve(
    initial_program_path="path/to/program.py",
    evaluation_file="path/to/evaluate.py", 
    config=config
)

best_program = await controller.run(iterations=100)

# Check meta prompt statistics
if controller.meta_prompt_db:
    print(f"Total meta prompts: {len(controller.meta_prompt_db.meta_prompts)}")
    
    best_meta_prompt = controller.meta_prompt_db.get_best_meta_prompt()
    print(f"Best meta prompt effectiveness: {best_meta_prompt.effectiveness_score}")
```

## Implementation Details

### Meta Prompt Selection

Meta prompts are selected using weighted sampling based on their effectiveness scores:

- **Effectiveness Score**: Average performance of programs generated by the meta prompt
- **Weighted Sampling**: Better meta prompts are more likely to be selected
- **Fallback**: If no meta prompts exist, uses the base system message

### Performance Tracking

The system tracks the relationship between meta prompts and generated programs:

- **Linkage**: Each program is linked to the meta prompt that helped create it
- **Metrics**: Meta prompt performance is updated when linked programs are evaluated
- **Aggregation**: Performance metrics are aggregated across all linked programs

### Meta Prompt Evolution Template

The system uses a specialized prompt template for evolving meta prompts:

```
# Current System Prompt Performance Analysis
## Current System Prompt
"[current prompt content]"

Performance Metrics:
- metric1: value
- metric2: value
...

## Performance Feedback
- iteration: current_iteration
- total_programs: program_count
- best_score: best_program_score
...

## Top Performing Programs Generated
[Code examples and metrics from best programs]

# Task
Based on the performance analysis above, create an improved system prompt 
that will guide the LLM to generate better programs...
```

### File Structure

The meta prompt evolution feature adds these new files:

- `openevolve/meta_prompt_evolution.py`: Core meta prompt evolution classes
- `examples/meta_prompt_evolution_example.py`: Usage example
- `examples/function_minimization_prompt_evolution/config.yaml`: Example config
- `META_PROMPT_EVOLUTION.md`: This documentation

## Logging and Monitoring

The system provides detailed logging for meta prompt evolution:

```
INFO - Initialized meta prompt evolution
DEBUG - Using meta prompt abc123 for system message  
INFO - Evolving meta prompts at iteration 20
INFO - Generated new meta prompt: def456...
DEBUG - Updated meta prompt abc123 performance: 0.742
```

## Limitations and Future Work

### Current Limitations

- Meta prompts are evolved independently (no crossover/mutation)
- Evolution interval is fixed (not adaptive)
- No persistence of meta prompt database across runs
- User prompts are not evolved (only system prompts)

### Future Enhancements

- **Adaptive Evolution**: Vary evolution frequency based on performance trends
- **Prompt Persistence**: Save/load meta prompt databases
- **User Prompt Evolution**: Extend to evolve user prompts as well
- **Advanced Selection**: Implement more sophisticated selection strategies
- **Performance Analytics**: Add detailed meta prompt performance analysis

## Research Background

This implementation is inspired by the meta prompt evolution concept from the AlphaEvolve paper:

> "Meta prompt evolution: instructions and context suggested by the LLM itself in an additional prompt-generation step, co-evolved in a separate database analogous to the solution programs."

The feature allows OpenEvolve to potentially surpass the performance obtainable using human-crafted prompts by automatically discovering better ways to instruct the LLM during code evolution.

## Contributing

When contributing to meta prompt evolution:

1. **Test Thoroughly**: Ensure meta prompt evolution doesn't break existing functionality
2. **Document Changes**: Update this README for any new features or changes
3. **Performance Impact**: Consider the computational overhead of meta prompt evolution
4. **Backward Compatibility**: Maintain compatibility with existing configs (disabled by default)

## See Also

- [OpenEvolve Main Documentation](README.md)
- [Configuration Guide](docs/configuration.md)
- [Examples Directory](examples/)
- [AlphaEvolve Paper](https://arxiv.org/abs/2406.20957) (for research background)