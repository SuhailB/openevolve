"""
Meta Prompt Evolution for OpenEvolve

Implements meta prompt evolution as described in AlphaEvolve paper.
Evolves system prompts based on performance feedback while keeping user prompts unchanged.
"""

import asyncio
import json
import logging
import time
import uuid
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Optional, Tuple

from openevolve.config import Config, PromptConfig
from openevolve.llm.ensemble import LLMEnsemble
from openevolve.database import Program

logger = logging.getLogger(__name__)


@dataclass
class MetaPrompt:
    """Represents a meta prompt (system prompt) in the database"""
    
    # Meta prompt identification
    id: str
    content: str  # The actual system prompt text
    
    # Evolution information
    parent_id: Optional[str] = None
    generation: int = 0
    timestamp: float = field(default_factory=time.time)
    iteration_found: int = 0
    
    # Performance metrics (aggregated from programs using this meta prompt)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    programs_generated: int = 0  # Number of programs generated using this meta prompt
    
    # Quality measures
    effectiveness_score: float = 0.0  # How effective this meta prompt is
    diversity_score: float = 0.0  # How diverse are programs it generates
    
    # Metadata
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MetaPrompt":
        """Create from dictionary representation"""
        return cls(**data)


class MetaPromptDatabase:
    """Database for storing and sampling meta prompts during evolution"""
    
    def __init__(self, config: PromptConfig):
        self.config = config
        
        # In-memory meta prompt storage
        self.meta_prompts: Dict[str, MetaPrompt] = {}
        
        # Track the best meta prompt
        self.best_meta_prompt_id: Optional[str] = None
        
        # Track which programs were generated by which meta prompts
        self.program_to_meta_prompt: Dict[str, str] = {}
        
        # Performance tracking
        self.meta_prompt_performance: Dict[str, List[float]] = {}
        # Raw metric values for averaging (separate from computed averages)
        self.meta_prompt_raw_metrics: Dict[str, Dict[str, List[float]]] = {}
        
        logger.info("Initialized MetaPromptDatabase")
    
    def add_meta_prompt(self, meta_prompt: MetaPrompt, iteration: int = None) -> str:
        """Add a meta prompt to the database"""
        if iteration is not None:
            meta_prompt.iteration_found = iteration
            
        self.meta_prompts[meta_prompt.id] = meta_prompt
        self.meta_prompt_performance[meta_prompt.id] = []
        self.meta_prompt_raw_metrics[meta_prompt.id] = {}
        
        # Update best meta prompt tracking
        self._update_best_meta_prompt(meta_prompt)
        
        logger.debug(f"Added meta prompt {meta_prompt.id}")
        return meta_prompt.id
    
    def get_meta_prompt(self, meta_prompt_id: str) -> Optional[MetaPrompt]:
        """Get a meta prompt by ID"""
        return self.meta_prompts.get(meta_prompt_id)
    
    def sample_meta_prompt(self) -> Optional[MetaPrompt]:
        """Sample a meta prompt for use in program generation"""
        if not self.meta_prompts:
            return None
            
        # Use weighted sampling with exploration bonus for newer meta prompts
        meta_prompts = list(self.meta_prompts.values())
        weights = []
        
        for mp in meta_prompts:
            # Base weight from effectiveness
            base_weight = mp.effectiveness_score + 0.1
            
            # Exploration bonus for newer generations (encourages trying evolved prompts)
            generation_bonus = 0.2 * mp.generation if mp.generation > 0 else 0.0
            
            # Small bonus for under-explored meta prompts
            exploration_bonus = 0.1 if mp.programs_generated < 5 else 0.0
            
            total_weight = base_weight + generation_bonus + exploration_bonus
            weights.append(total_weight)
        
        import random
        selected = random.choices(meta_prompts, weights=weights, k=1)[0]
        
        logger.debug(f"Sampled meta prompt {selected.id} (gen: {selected.generation}) "
                    f"with effectiveness {selected.effectiveness_score:.3f}")
        return selected
    
    def link_program_to_meta_prompt(self, program_id: str, meta_prompt_id: str):
        """Link a generated program to the meta prompt that helped create it"""
        self.program_to_meta_prompt[program_id] = meta_prompt_id
        
        # Increment usage counter
        if meta_prompt_id in self.meta_prompts:
            self.meta_prompts[meta_prompt_id].programs_generated += 1
    
    def update_meta_prompt_performance(self, program: Program):
        """Update meta prompt performance based on program results"""
        program_id = program.id
        if program_id not in self.program_to_meta_prompt:
            return
            
        meta_prompt_id = self.program_to_meta_prompt[program_id]
        if meta_prompt_id not in self.meta_prompts:
            return
            
        # Calculate program fitness (average of all metrics)
        if program.metrics:
            fitness = sum(program.metrics.values()) / len(program.metrics)
        else:
            fitness = 0.0
            
        # Update meta prompt performance
        self.meta_prompt_performance[meta_prompt_id].append(fitness)
        
        # Update aggregated metrics
        meta_prompt = self.meta_prompts[meta_prompt_id]
        performances = self.meta_prompt_performance[meta_prompt_id]
        
        meta_prompt.effectiveness_score = sum(performances) / len(performances)
        
        # Update raw performance metrics (keep lists separate from computed averages)
        if meta_prompt_id not in self.meta_prompt_raw_metrics:
            self.meta_prompt_raw_metrics[meta_prompt_id] = {}
            
        for metric_name, value in program.metrics.items():
            if metric_name not in self.meta_prompt_raw_metrics[meta_prompt_id]:
                self.meta_prompt_raw_metrics[meta_prompt_id][metric_name] = []
            self.meta_prompt_raw_metrics[meta_prompt_id][metric_name].append(value)
        
        # Calculate average performance metrics (store as computed averages)
        meta_prompt.performance_metrics = {}
        for metric_name, values in self.meta_prompt_raw_metrics[meta_prompt_id].items():
            if values:  # Ensure we have values
                meta_prompt.performance_metrics[metric_name] = sum(values) / len(values)
        
        self._update_best_meta_prompt(meta_prompt)
        
        logger.debug(f"Updated meta prompt {meta_prompt_id} performance: {meta_prompt.effectiveness_score:.3f}")
    
    def _update_best_meta_prompt(self, meta_prompt: MetaPrompt):
        """Update tracking of the best meta prompt"""
        if (self.best_meta_prompt_id is None or 
            meta_prompt.effectiveness_score > self.meta_prompts[self.best_meta_prompt_id].effectiveness_score):
            self.best_meta_prompt_id = meta_prompt.id
            logger.debug(f"New best meta prompt: {meta_prompt.id} (score: {meta_prompt.effectiveness_score:.3f})")
    
    def get_best_meta_prompt(self) -> Optional[MetaPrompt]:
        """Get the best performing meta prompt"""
        if self.best_meta_prompt_id:
            return self.meta_prompts[self.best_meta_prompt_id]
        return None
    
    def get_top_meta_prompts(self, n: int = 5) -> List[MetaPrompt]:
        """Get top N performing meta prompts"""
        meta_prompts = list(self.meta_prompts.values())
        meta_prompts.sort(key=lambda mp: mp.effectiveness_score, reverse=True)
        return meta_prompts[:n]


class MetaPromptEvolver:
    """Evolves meta prompts using LLM feedback"""
    
    def __init__(self, llm_ensemble: LLMEnsemble, config: PromptConfig):
        self.llm_ensemble = llm_ensemble
        self.config = config
        
        logger.info("Initialized MetaPromptEvolver")
    
    async def evolve_meta_prompt(self, 
                                 current_meta_prompt: Optional[MetaPrompt],
                                 top_programs: List[Program],
                                 performance_feedback: Dict[str, Any]) -> MetaPrompt:
        """Evolve a meta prompt based on performance feedback"""
        
        # Build the meta prompt evolution prompt
        evolution_prompt = self._build_meta_prompt_evolution_prompt(
            current_meta_prompt, top_programs, performance_feedback
        )
        
        # Generate new meta prompt using LLM
        system_message = self._get_meta_prompt_evolution_system_message()
        messages = [{"role": "user", "content": evolution_prompt}]
        
        response = await self.llm_ensemble.generate_with_context(
            system_message=system_message,
            messages=messages
        )
        
        # Extract the new meta prompt from response
        new_content = self._extract_meta_prompt_from_response(response)
        
        # Create new MetaPrompt object with inherited effectiveness score
        # Give evolved meta prompts a slight boost to encourage exploration
        if current_meta_prompt:
            # Inherit parent's effectiveness with small boost for exploration
            starting_effectiveness = min(current_meta_prompt.effectiveness_score + 0.1, 1.0)
        else:
            # First evolution, start with neutral score
            starting_effectiveness = 0.5
            
        new_meta_prompt = MetaPrompt(
            id=str(uuid.uuid4()),
            content=new_content,
            parent_id=current_meta_prompt.id if current_meta_prompt else None,
            generation=(current_meta_prompt.generation + 1) if current_meta_prompt else 0,
            effectiveness_score=starting_effectiveness
        )
        
        logger.info(f"Evolved new meta prompt {new_meta_prompt.id} "
                   f"(gen: {new_meta_prompt.generation}, "
                   f"starting effectiveness: {new_meta_prompt.effectiveness_score:.3f})")
        return new_meta_prompt
    
    def _build_meta_prompt_evolution_prompt(self,
                                            current_meta_prompt: Optional[MetaPrompt],
                                            top_programs: List[Program],
                                            performance_feedback: Dict[str, Any]) -> str:
        """Build the prompt for evolving meta prompts"""
        
        prompt_parts = []
        
        prompt_parts.append("# Current System Prompt Performance Analysis")
        
        if current_meta_prompt:
            prompt_parts.append(f"## Current System Prompt")
            prompt_parts.append(f'"{current_meta_prompt.content}"')
            prompt_parts.append(f"")
            prompt_parts.append(f"Performance Metrics:")
            if current_meta_prompt.performance_metrics:
                for metric, value in current_meta_prompt.performance_metrics.items():
                    prompt_parts.append(f"- {metric}: {value:.4f}")
            else:
                prompt_parts.append("- No performance data yet")
            prompt_parts.append(f"Effectiveness Score: {current_meta_prompt.effectiveness_score:.4f}")
            prompt_parts.append(f"Programs Generated: {current_meta_prompt.programs_generated}")
        else:
            prompt_parts.append("## Current System Prompt")
            prompt_parts.append("No previous system prompt (this is the first evolution)")
        
        prompt_parts.append(f"")
        prompt_parts.append(f"## Performance Feedback")
        for key, value in performance_feedback.items():
            prompt_parts.append(f"- {key}: {value}")
        
        if top_programs:
            prompt_parts.append(f"")
            prompt_parts.append(f"## Top Performing Programs Generated")
            for i, program in enumerate(top_programs[:3], 1):
                prompt_parts.append(f"### Program {i}")
                if program.metrics:
                    metrics_str = ", ".join(f"{k}={v:.3f}" for k, v in program.metrics.items())
                    prompt_parts.append(f"Metrics: {metrics_str}")
                prompt_parts.append(f"Code snippet:")
                prompt_parts.append(f"```{program.language}")
                # Show first 10 lines of code
                code_lines = program.code.split('\n')[:10]
                prompt_parts.append('\n'.join(code_lines))
                if len(program.code.split('\n')) > 10:
                    prompt_parts.append("... (truncated)")
                prompt_parts.append(f"```")
                prompt_parts.append("")
        
        prompt_parts.append("# Task")
        prompt_parts.append("Based on the performance analysis above, create an improved system prompt that will guide the LLM to generate better programs.")
        prompt_parts.append("")
        prompt_parts.append("The new system prompt should:")
        prompt_parts.append("1. Address weaknesses identified in the current approach")
        prompt_parts.append("2. Build upon successful patterns from top-performing programs")
        prompt_parts.append("3. Provide clearer guidance for program improvement")
        prompt_parts.append("4. Maintain focus on the core optimization task")
        prompt_parts.append("")
        prompt_parts.append("Provide ONLY the new system prompt text, without any additional explanation or formatting.")
        
        return '\n'.join(prompt_parts)
    
    def _get_meta_prompt_evolution_system_message(self) -> str:
        """Get the system message for meta prompt evolution"""
        return """You are an expert prompt engineer specializing in creating effective system prompts for code optimization tasks.

Your role is to analyze the performance of current system prompts and create improved versions that will guide language models to generate better, more optimized code.

You understand:
- How different prompt formulations affect code generation quality
- The importance of clear, specific guidance in system prompts
- How to incorporate performance feedback into prompt improvements
- The balance between being directive and allowing creative solutions

Focus on creating system prompts that are:
- Clear and specific about the optimization goals
- Encouraging of innovative approaches
- Balanced between guidance and flexibility
- Focused on measurable performance improvements"""
    
    def _extract_meta_prompt_from_response(self, response: str) -> str:
        """Extract the meta prompt content from LLM response"""
        # Simple extraction - just clean up the response
        content = response.strip()
        
        # Remove any markdown formatting if present
        if content.startswith('```') and content.endswith('```'):
            lines = content.split('\n')
            content = '\n'.join(lines[1:-1])
        
        # Remove quotes if the entire content is quoted
        if content.startswith('"') and content.endswith('"'):
            content = content[1:-1]
        
        return content.strip()


def create_initial_meta_prompt(base_system_message: str) -> MetaPrompt:
    """Create the initial meta prompt from the base system message"""
    return MetaPrompt(
        id=str(uuid.uuid4()),
        content=base_system_message,
        generation=0,
        effectiveness_score=0.5  # Neutral starting score
    )