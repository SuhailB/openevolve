#!/usr/bin/env python3
"""
Example of using meta prompt evolution with OpenEvolve

This example shows how to enable and configure meta prompt evolution
to automatically improve system prompts during the evolution process.
"""

import asyncio
import os
from openevolve import OpenEvolve
from openevolve.config import Config


async def main():
    """Run meta prompt evolution example"""
    
    # Path to the initial program and evaluation script
    initial_program_path = "function_minimization/initial_program.py"
    evaluation_file = "function_minimization/evaluate.py"
    
    # Create config with meta prompt evolution enabled
    config = Config()
    
    # Enable meta prompt evolution
    config.prompt.use_meta_prompting = True
    config.prompt.meta_prompt_evolution_interval = 20  # Evolve every 20 iterations
    config.prompt.system_message = (
        "You are an expert programmer specializing in optimization algorithms. "
        "Your task is to improve code to achieve better performance metrics."
    )
    
    # Configure evolution parameters
    config.max_iterations = 100
    config.database.population_size = 50
    config.database.archive_size = 20
    
    # Initialize OpenEvolve
    controller = OpenEvolve(
        initial_program_path=initial_program_path,
        evaluation_file=evaluation_file,
        config=config,
        output_dir="meta_prompt_evolution_output"
    )
    
    print("Starting evolution with meta prompt evolution enabled...")
    print(f"Meta prompts will evolve every {config.prompt.meta_prompt_evolution_interval} iterations")
    
    # Run evolution
    best_program = await controller.run(iterations=100)
    
    if best_program:
        print(f"\nBest program found:")
        print(f"- ID: {best_program.id}")
        print(f"- Metrics: {best_program.metrics}")
        print(f"- Generation: {best_program.generation}")
        
        # Show meta prompt evolution stats
        if controller.meta_prompt_db:
            print(f"\nMeta prompt evolution stats:")
            print(f"- Total meta prompts: {len(controller.meta_prompt_db.meta_prompts)}")
            
            best_meta_prompt = controller.meta_prompt_db.get_best_meta_prompt()
            if best_meta_prompt:
                print(f"- Best meta prompt effectiveness: {best_meta_prompt.effectiveness_score:.3f}")
                print(f"- Programs generated by best meta prompt: {best_meta_prompt.programs_generated}")
    else:
        print("No best program found")


if __name__ == "__main__":
    asyncio.run(main())